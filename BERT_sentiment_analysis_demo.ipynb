{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/bert-2phase.jpg\">\n",
    "<caption><center>BERT model for transfer learning: use a pre-trained BERT model and fine tune for your downstream NLP tasks</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT pre-trained model on PyTorch Hub:\n",
    "\n",
    "bert-base-chinese、bert-base-uncased、bert-base-cased、bert-base-german-cased、bert-base-multilingual-uncased、bert-base-multilingual-cased、bert-large-cased、bert-large-uncased、bert-large-uncased-whole-word-masking、bert-large-cased-whole-word-masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from IPython.display import clear_output\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"\n",
    "#PRETRAINED_MODEL_NAME = 'bert-base-cased'\n",
    "#PRETRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "\n",
    "# Download BERT Chinese tokenizer model\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "\n",
    "print(\"PyTorch version: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers as t\n",
    "t.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.vocab\n",
    "print(\"Size of the dictionary: \", len(vocab))\n",
    "\n",
    "type(vocab)\n",
    "#list(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the tokens in Chinese BERT dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random_tokens = random.sample(list(vocab), 10)\n",
    "random_ids = [vocab[t] for t in random_tokens]\n",
    "\n",
    "print(\"{0:20}{1:15}\".format(\"token\", \"index\"))\n",
    "print(\"-\" * 25)\n",
    "for t, id in zip(random_tokens, random_ids):\n",
    "    print(\"{0:15}{1:10}\".format(t, id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT uses the WordPiece Tokenization developed with Google NMT. Wordpiece tokens are with prefix \"##\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taiwan's phonetic symbol, bopomofo is also collected in the Chinese BERT dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(647, 666))\n",
    "some_pairs = [(t, idx) for t, idx in vocab.items() if idx in indices]\n",
    "for pair in some_pairs:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize with BERT tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize('九到十二個月')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"神愛世人，甚至將祂的獨生子賜給他們，叫一切信祂的，不至滅亡，反得永生。（約翰福音3:16）\"\n",
    "#text = \"起初，神創造天地。地是空虛混沌，淵面黑暗。神的靈運行在水面上，神說：「要有光」，就有了光。（創世紀1:1~3）\"\n",
    "#text = \"愛是恆久忍耐，又有恩慈；愛是不嫉妒；愛是不自誇，不張狂，不做害羞的事，不求自己的益處，不輕易發怒，不計算人的惡，不喜歡不義，只喜歡真理；凡事包容，凡事相信，凡事盼望，凡事忍耐。愛是永不止息。（哥前13:4~8a）\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(text)\n",
    "print(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are five special tokens in BERT:\n",
    "\n",
    "[CLS]：representation for the classification of the input sequence.\n",
    "\n",
    "[SEP]：boundary of the two input sequences.\n",
    "\n",
    "[UNK]：token for wordpieces not in the BERT dictionary.\n",
    "\n",
    "[PAD]：zero padding in one batch.\n",
    "\n",
    "[MASK]：mask token used in the Masked Language Model task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT is a very powerful language representation model that could be used in many downstream NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Prepare text dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = 'data/training_data.txt'\n",
    "lines = open(data_source, 'r', encoding='UTF-8').read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "text = []\n",
    "\n",
    "for l in lines:\n",
    "    label.append(l.split(' +++$+++ ')[0])\n",
    "    text.append(l.split(' +++$+++ ')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_dic = {\n",
    "    \"text\": text, \n",
    "    \"label\": label,\n",
    "}\n",
    "df_train = pd.DataFrame(data_dic)\n",
    "\n",
    "# Save the training dataset to tsv file for PyTorch\n",
    "df_train.to_csv(\"data/train.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_source = 'data/testing_data.csv'\n",
    "lines = open(test_data_source, 'r', encoding='UTF-8').read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label = []\n",
    "test_text = []\n",
    "\n",
    "for l in lines[1:]:\n",
    "    test_label.append(l[0])\n",
    "    test_text.append(l[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_dic = {\n",
    "    \"text\": test_text, \n",
    "    \"label\": test_label,\n",
    "}\n",
    "df_test = pd.DataFrame(test_dic)\n",
    "\n",
    "# Save the test dataset to tsv file for PyTorch\n",
    "df_test.to_csv(\"data/test.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove null examples\n",
    "empty_title = ((df_train['text'].isnull()) | (df_train['text'] == ''))\n",
    "df_train = df_train[~empty_title]\n",
    "\n",
    "# Remove the sequences longer than 60 tokens\n",
    "MAX_LENGTH = 60\n",
    "df_train = df_train[~(df_train.text.apply(lambda x : len(x)) > MAX_LENGTH)]\n",
    "\n",
    "# Use 20% of the training dataset\n",
    "SAMPLE_FRAC = 0.2\n",
    "df_train = df_train.sample(frac=SAMPLE_FRAC, random_state=9527)\n",
    "\n",
    "# Remove unused columns and rename the used columns\n",
    "df_train = df_train.reset_index()\n",
    "df_train = df_train.loc[:, ['text', 'label']]\n",
    "df_train.columns = ['text', 'label']\n",
    "\n",
    "print(\"Number of examples of the training dataset: \", len(df_train))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.label.value_counts() / len(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of examples of the test dataset: \", len(df_test))\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = len(df_test) / len(df_train)\n",
    "print(f\"Size of test set:  / Size of training set:  = {ratio:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Convert the text data into BERT compatible format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three kinds of tensors:\n",
    "\n",
    "tokens_tensor：index value in the BERT dictionary of every input token\n",
    "\n",
    "segments_tensor：used to distinguish the two input sequences. The first sequence is 0, the second is 1.\n",
    "\n",
    "masks_tensor：masks of the self-attention mechnism. 1 for tokens to be attended and 0 for padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read the dataset and convert the sentences to BERT compatible format.\n",
    "It returns 2 tensors：\n",
    "- tokens_tensor：an index sequence of the sentence include [CLS].\n",
    "- label_tensor：classification label, none for test set\n",
    "- 0：Negtive sentiment\n",
    "- 1：Positive sentiment\n",
    "\"\"\"\n",
    "from torch.utils.data import Dataset\n",
    "import pysnooper\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, mode, tokenizer):\n",
    "        assert mode in [\"train\", \"test\"]\n",
    "        self.mode = mode\n",
    "        self.df = pd.read_csv(\"data/\" + mode + \".tsv\", sep=\"\\t\").fillna(\"\")\n",
    "        self.len = len(self.df)\n",
    "        self.label_map = {'0': 'Negtive', '1': 'Positive'}\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #if self.mode == \"test\":\n",
    "        #    text = self.df.iloc[idx, 0]\n",
    "        #    label_tensor = None\n",
    "        #else:\n",
    "        text, label = self.df.iloc[idx, :].values\n",
    "        label_tensor = torch.tensor(int(label))\n",
    "\n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        word_pieces += tokens\n",
    "        len_a = len(word_pieces)\n",
    "\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "\n",
    "        return (tokens_tensor, label_tensor)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    \n",
    "# Initialize a training dataset and use BERT to tokenize.\n",
    "trainset = SentimentDataset(\"train\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one sample.\n",
    "sample_idx = 41\n",
    "\n",
    "# Original sample in dataset\n",
    "text, label = trainset.df.iloc[sample_idx].values\n",
    "\n",
    "# Take one sample from the training dataset\n",
    "tokens_tensor, label_tensor = trainset[sample_idx]\n",
    "\n",
    "# Convert the tokens_tensor into text\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())\n",
    "combined_text = \" \".join(tokens)\n",
    "\n",
    "print(f\"\"\"[Original text]\n",
    "Sentence: {text}\n",
    "Classification: {trainset.label_map[str(label)]}\n",
    "--------------------\n",
    "[Returned tensors from dataset]\n",
    "tokens_tensor  ：{tokens_tensor}\n",
    "label_tensor   ：{label_tensor}\n",
    "--------------------\n",
    "[Converted from tokens_tensors]\n",
    "{combined_text}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini-batches returned from a DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reads the SentimentDataset and returns 3 tensors required by BERT\n",
    "- tokens_tensors  : (batch_size, max_seq_len_in_batch)\n",
    "- masks_tensors   : (batch_size, max_seq_len_in_batch)\n",
    "- label_ids       : (batch_size)\n",
    "\"\"\"\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    \n",
    "    if samples[0][1] is not None:\n",
    "        label_ids = torch.stack([s[1] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    tokens_tensors = pad_sequence(tokens_tensors, \n",
    "                                  batch_first=True)\n",
    "\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, \n",
    "                                dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(\n",
    "        tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, masks_tensors, label_ids\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, \n",
    "                         collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = next(iter(trainloader))\n",
    "\n",
    "tokens_tensors, masks_tensors, label_ids = data\n",
    "\n",
    "print(f\"\"\"\n",
    "tokens_tensors.shape   = {tokens_tensors.shape} \n",
    "{tokens_tensors}\n",
    "------------------------\n",
    "masks_tensors.shape    = {masks_tensors.shape}\n",
    "{masks_tensors}\n",
    "------------------------\n",
    "label_ids.shape        = {label_ids.shape}\n",
    "{label_ids}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Downstream NLP tasks based on pre-trained BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/bert_fine_tuning_tasks.jpg\">\n",
    "<caption><center>4 NLP tasks based on a fine-tuned BERT model</center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "PRETRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "NUM_LABELS = 2\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "\n",
    "print(\"\"\"\n",
    "name            module\n",
    "----------------------\"\"\")\n",
    "for name, module in model.named_children():\n",
    "    if name == \"bert\":\n",
    "        for n, _ in module.named_children():\n",
    "            print(f\"{name}:{n}\")\n",
    "    else:\n",
    "        print(\"{:15} {}\".format(name, module))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default parameters of the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the parameter value of the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.hidden_size = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.architectures = 'bertForMaskedLM'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, there are 8 models and 1 tokenizer on PyTorch Hub.\n",
    "\n",
    "Basic:\n",
    "bertModel、\n",
    "bertTokenizer\n",
    "\n",
    "Pretrained model:\n",
    "bertForMaskedLM、\n",
    "bertForNextSentencePrediction、\n",
    "bertForPreTraining\n",
    "\n",
    "Fine-tuning for downstream NLP tasks based on BERT:\n",
    "bertForSequenceClassification、\n",
    "bertForTokenClassification、\n",
    "bertForQuestionAnswering、\n",
    "bertForMultipleChoice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Please refer to https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html\n",
    "\"\"\"\n",
    "\n",
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "      \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for data in dataloader:\n",
    "            # Move tensors to GPU\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "\n",
    "            # First 2 tensors are token tensor and mask tensor\n",
    "            tokens_tensors, masks_tensors = data[:2]\n",
    "            outputs = model(input_ids=tokens_tensors, attention_mask=masks_tensors)\n",
    "            \n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            \n",
    "            # Compute accuracy\n",
    "            if compute_acc:\n",
    "                labels = data[2]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            # Record prediction results\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "    \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "    return predictions\n",
    "    \n",
    "# Running the model on GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)\n",
    "#_, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "#print(\"classification acc:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get total learnable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learnable_params(module):\n",
    "    return [p for p in module.parameters() if p.requires_grad]\n",
    "     \n",
    "model_params = get_learnable_params(model)\n",
    "clf_params = get_learnable_params(model.classifier)\n",
    "\n",
    "print(f\"\"\"\n",
    "Total parameters of the whole model: {sum(p.numel() for p in model_params)}\n",
    "Total parameters of the linear classifier: {sum(p.numel() for p in clf_params)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Training and fine tuning for your downstream NLP tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Training mode\n",
    "model.train()\n",
    "\n",
    "# Use Adam Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "EPOCHS = 30\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for data in trainloader:\n",
    "        \n",
    "        tokens_tensors, masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "        # Initialize the gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(input_ids=tokens_tensors, attention_mask=masks_tensors, labels=labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # Record batch loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    # Calculate accuracy\n",
    "    _, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "\n",
    "    print('[epoch %d] loss: %.3f, acc: %.3f' %\n",
    "          (epoch + 1, running_loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.save(model, 'BERT_sentiment_analysis.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('BERT_sentiment_analysis.pkl', map_location = torch.device(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Inference for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = SentimentDataset(\"test\", tokenizer=tokenizer)\n",
    "sample_idx = 10\n",
    "\n",
    "# Original data in test dataset\n",
    "text, label = testset.df.iloc[sample_idx].values\n",
    "test_tokens_tensor, _ = testset[sample_idx]\n",
    "tokens = tokenizer.convert_ids_to_tokens(test_tokens_tensor.tolist())\n",
    "combined_text = \" \".join(tokens)\n",
    "print(f\"\"\"[Original data]\n",
    "Sentence: {text}\n",
    "Label: {testset.label_map[str(label)]}\n",
    "--------------------\n",
    "[Returned tensors by dataset]\n",
    "tokens_tensor  ：{tokens_tensor}\n",
    "--------------------\n",
    "[Converted tokens_tensors]\n",
    "{combined_text}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "# Test dataset and data loader\n",
    "testset = SentimentDataset(\"test\", tokenizer=tokenizer)\n",
    "testloader = DataLoader(testset, batch_size=16, \n",
    "                        collate_fn=create_mini_batch)\n",
    "\n",
    "# Prediction\n",
    "#predictions, acc = get_predictions(model, testloader, compute_acc = True)\n",
    "\n",
    "index_map = {int(k): v for k, v in testset.label_map.items()}\n",
    "\n",
    "#df = pd.DataFrame({\"Category\": predictions.tolist()})\n",
    "#df['Category'] = df.Category.apply(lambda x: index_map[x])\n",
    "#df_pred = pd.concat([testset.df.loc[:, [\"label\"]], \n",
    "#                          df.loc[:, 'Category']], axis=1)\n",
    "\n",
    "#print(f'Accuracy: {acc:.3f}')\n",
    "\n",
    "#df_pred.to_csv('data/bert_sentiment_predict.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_pred.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment prediction per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentTestData(Dataset):\n",
    "    \n",
    "    def __init__(self, text_in, tokenizer):\n",
    "        self.label_map = {'0': 'Negtive', '1': 'Positive'}\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = pd.DataFrame({\"text\": text_in})\n",
    "        self.len = len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        text = self.df.iloc[idx, 0]\n",
    "        label_tensor = None\n",
    "        \n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        word_pieces += tokens\n",
    "\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "\n",
    "        return (tokens_tensor, label_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_in = [text]\n",
    "test_data = SentimentTestData(text_in, tokenizer=tokenizer)\n",
    "test_data_loader = DataLoader(test_data, batch_size = 1, collate_fn=create_mini_batch)\n",
    "predictions = get_predictions(model, test_data_loader)\n",
    "print(f\"The predicted sentiment is {index_map[predictions.tolist()[0]]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
